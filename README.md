# ğŸ‘“ Ray-Ban Price Scanner â€” AR-first demo

A quick prototype of whatâ€™s possible with Ray-Ban Meta glasses as an ambient shopping companion: glance at a barcode and a phone instantly returns price and context. Itâ€™s intentionally narrow, focused on proving the loop (see â†’ know â†’ act) and hinting at where AR utility could go next.

## Why I built it
- A layoff + a brand-new Meta wearables SDK + a pair of Ray-Ban Meta glasses sent me down an AR rabbit hole I didnâ€™t expect.
- Iâ€™m frugal and constantly price-checkingâ€”so the question became: what if I could just look at something and instantly see the price?


## AI-assisted build
I built roughly 95% of this entire project with ChatGPTâ€”and it was incredible! Watching an idea turn into a plan and then into a working prototype, with AI doing most of the heavy lifting, felt downright magical. This is a huge leap forward for product managers: we can now rapidly prototype ideas at lightning speed before ever looping in engineering for full-scale development. It honestly feels like having a supercharged 3D printer for softwareâ€”the barrier to fast, creative experimentation has never been lower, and itâ€™s unbelievably exciting.


## Screenshots & demos
- Live Stream UI (status pill, overlays): `docs/screenshots/live-stream.png`  
- Barcode overlay with tooltip: `docs/screenshots/overlay-tooltip.png`  
- Phone scanner flow: `docs/screenshots/phone-scan.png`  
- Demo previews (full-length GIF) stacked:

**Live Preview Boudning Box with Price Return**

<img src="docs/demos/LiveScanDemoWithtBoundingBox.gif" alt="Live scan demo" width="320"/><br/>
[Full video](docs/demos/LiveScanDemoWithtBoundingBox.mp4)

**Scan with Meta Glasses and Return Price on UI**

<img src="docs/demos/RaybanScanToPriceDemo.gif" alt="Meta UI scan demo" width="320"/><br/>
[Full video](docs/demos/RaybanScanToPriceDemo.mp4)


## Feature highlights
- ğŸ‘“ğŸ“± Glasses + phone scanning, plus photo-library scanning for debugging.
- ğŸ¥ Live Stream viewer with still capture, save/share, menu-driven FPS (no auto-restart), and Vision barcode overlays.
- ğŸŸ¡ Live overlay auto-detection: highlights barcodes in the preview and auto-runs a lookup; tooltip shows title/price when found.
- ğŸ”— Lookup chain: UPCItemDB â†’ BarcodeLookup (API key) â†’ Barcode Monster â†’ OpenFoodFacts (first useful hit wins).
- ğŸ› ï¸ CLI detector: `Scripts/BarcodeTest.swift` to exercise the Vision/OCR stack on still images.

## Setup
1. Install Meta Wearables SDK (`MWDATCore`, `MWDATCamera`) per Meta docs.  
2. Open `RayBanPriceScanner/RayBanPriceScanner.xcodeproj`, set signing, adjust bundle ID if needed.  
3. In `Info.plist`, set `MWDAT.MetaAppID`, align `MWDAT.AppLinkURLScheme` with your bundle ID + `://`, optionally add `BARCODE_LOOKUP_KEY` for richer pricing.  
4. Build/run on device. In-app Registration pairs the glasses via Meta AI (Developer Mode on), then tap Play in Live Stream.

## How to use
- ğŸ“± Phone: â€œScan with iPhone Cameraâ€ (AVCapture + Vision + OCR fallback) â†’ lookup + speech.  
- ğŸ‘“ Glasses: Live Stream â†’ Play to pull frames, capture a high-quality still, see barcode overlays, and get auto tooltip lookups. FPS via the speedometer menu; no auto-reconnects.  
- ğŸ–¼ï¸ Photos: â€œPick Photoâ€ runs the same Vision/OCR pipeline on saved images.

## Design guardrails
- ğŸ™… User-agency: start/stop throttled to avoid churn; FPS changes stop the stream until Play is pressed.
- ğŸ›¡ï¸ Stability: explicit Reset button, minimal logging by default.
- ğŸ” Transparency: combined status pill (state + resolution + fps + auto/manual) and live barcode outlines/tooltips.

## Future directions
- ğŸ¤– Beyond barcodes: gentle object/framing cues to guide capture.  
- âš¡ Smarter capture policy: auto-still only when confidence spikes, tuned for thermals/battery.  
- ğŸ›’ Context: price comparisons, â€œbetter price nearby,â€ or â€œsave for laterâ€ once confidence is high.

## Known limitations
- Barcode detection is tuned for larger/clear barcodes; small/low-contrast codes may miss.
- â€œHey Metaâ€ voice triggers are not integrated; registration and streaming are initiated in-app.
- The current Meta SDK canâ€™t render price/context on-glasses; the app speaks the result on phone instead.

## Requirements
- Xcode 15+ and current iOS SDK (deployment target ~26.1; lower in Project Settings if needed).  
- Ray-Ban Meta smart glasses + Meta Wearables SDK.  
- iOS device with camera/Bluetooth/mic/photo permissions granted.

## CLI detector
```sh
swift -D BARCODE_SCRIPT RayBanPriceScanner/RayBanPriceScanner/Scripts/BarcodeTest.swift /path/to/image.jpg
```
Add `VERBOSE=1` to see Vision observations.

## Architecture (high level)
```
Ray-Ban Glasses â”€(MWDAT)â†’ StreamSessionViewModel â†’ LiveStreamView (UI)
                                   â”‚                  â”‚
                                   â”‚                  â”œâ”€ Vision overlay + tooltip lookup
                                   â”‚                  â””â”€ User controls (Play/Stop/FPS/Reset)
                                   â–¼
                            ContentView / QRScannerView (phone scan)
                                   â”‚
                                   â–¼
                         ProductLookupService (multi-provider)
                                   â”‚
                                   â–¼
                       ProductInfo â†’ SpeechService â†’ UI status
```



## Notes
- `ProductLookupService` trims UPC input and stops at the first provider with data; swap providers as needed.  
- Lightweight by design: this repo is for showcasing AR product thinking, not a full commerce stack.  
- Built with Apple Vision/AVFoundation/SwiftUI and the Meta Wearables SDK (MWDATCore/MWDATCamera).
